# deepcrawler.ai

> **An intelligent web crawler & parser powered by Streamlit, Selenium, and local LLMs with LangChain + Ollama**

---

## ğŸš€ Overview

**deepcrawler.ai** is a simple yet powerful web-based application that:

- Scrapes website content using **Selenium + BeautifulSoup**
- Cleans and splits DOM content intelligently
- Uses **LangChain + Ollama** to parse and extract structured data based on user queries
- Is built entirely in **Python with Streamlit**, making it fast, interactive, and easy to deploy

---

## ğŸ¯ Features

âœ… Web scraping with headless Chrome (Selenium)  
âœ… DOM cleaning and content extraction  
âœ… Smart chunking of large pages  
âœ… User-prompted content parsing via **local LLMs (Llama3 via Ollama)**  
âœ… Streamlit UI for interactive use  
âœ… Modular, extensible codebase

---

## ğŸ› ï¸ Tech Stack

| Tool           | Purpose                         |
|----------------|----------------------------------|
| `Streamlit`    | Web UI                           |
| `Selenium`     | Web scraping                     |
| `BeautifulSoup`| HTML parsing                     |
| `LangChain`    | LLM interface                    |
| `langchain_ollama` | Integration with local Ollama |
| `Ollama`       | Run Llama3 or other LLMs locally |

---

## ğŸ“¸ Screenshot

![deepcrawler-ui](img.jpeg)


---

## ğŸ“‚ Project Structure

```bash
deepcrawler/
â”œâ”€â”€ main.py           # Streamlit UI
â”œâ”€â”€ scrape.py         # Scraping & cleaning logic
â”œâ”€â”€ parse.py          # LangChain + Ollama parsing
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ .env              # Optional: Environment variables
```

---

## ğŸ§ª How It Works

1. **Input a website URL**
2. **Scrape HTML content** using Selenium
3. **Extract & clean** the DOM (body content only)
4. **Split** content into manageable chunks
5. **Describe** what you want to extract (e.g., "Extract all product names and prices")
6. **Ollama + LangChain** processes and parses the data

---

## ğŸš€ Getting Started

### ğŸ“¦ 1. Install Requirements

```bash
pip install -r requirements.txt
```

### ğŸ§  2. Run Ollama with your preferred model

```bash
ollama run llama3:8b
```

> Ensure Ollama is running locally at \`http://localhost:11434\` (default)

### â–¶ï¸ 3. Launch the App

```bash
streamlit run main.py
```

---

## ğŸŒ Deployment

You can deploy this app easily on:

- **Render.com** (recommended for full stack apps with Ollama)
- **Localhost** or private servers
- *(Not compatible with Hugging Face Spaces due to Ollama dependency)*

---

## ğŸ” Environment Variables

You can optionally set these in a \`.env\` file:

```env
OLLAMA_BASE_URL=http://localhost:11434
```

---

## ğŸ’¡ Use Cases

- Web data extraction
- AI-powered content parsing
- Open-source RAG (Retrieval-Augmented Generation) backend
- Web audit tools
- SEO or eCommerce scrapers

---

## ğŸ¤ Contributing

Pull requests, feedback, and ideas are welcome!  
Letâ€™s build the most powerful web crawler with AI together.

---

## ğŸ“„ License

MIT License â€“ do whatever you want, but please credit the project if it helps you!

---

## ğŸ™‹â€â™‚ï¸ Author

**aranyaadheu**  
ğŸ“ Sylhet, Bangladesh  
ğŸŒ [Portfolio](https://aranyaadheu.vercel.app) | ğŸ™ [GitHub](https://github.com/aranyaadheu)
